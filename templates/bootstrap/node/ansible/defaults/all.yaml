---
all:
  children:
    kiss_ephemeral_node:
      vars: {}
    kube_control_plane:
    kube_node:
    calico_rr:
      hosts: {}
    etcd:
    k8s_cluster_default:
      vars: {}
    k8s_cluster:
      children:
        kube_control_plane:
        kube_node:
      vars:
        #############################
        # KISS
        #############################

        # Optimize for HPC clusters
        kiss_hpc_enable: false

        #############################
        # k8s-cluster
        #############################

        # Kubernetes configuration dirs and system namespace.
        # Those are where all the additional config stuff goes
        # the kubernetes normally puts in /srv/kubernetes.
        # This puts them in a sane location and namespace.
        # Editing those values will almost surely break something.
        kube_config_dir: /etc/kubernetes

        ### fail with swap on (default true)
        kubelet_fail_swap_on: true

        # An alternative flexvolume plugin directory
        kubelet_flexvolumes_plugins_dir: /var/lib/kubelet/volumeplugins

        ## Container runtime
        ## docker for docker, crio for cri-o and containerd for containerd.
        ## Additionally you can set this to kubeadm if you want to install etcd using kubeadm
        ## Kubeadm etcd deployment is experimental and only available for new deployments
        ## If this is not set, container manager will be inherited from the Kubespray defaults
        ## and not from k8s_cluster/k8s-cluster.yml, which might not be what you want.
        ## Also this makes possible to use different container manager for etcd nodes.
        container_manager: containerd

        containerd_base_runtime_spec_rlimit_nofile: 1048576

        # Containerd conf default dir
        containerd_storage_dir: "/var/lib/containerd"

        ## An obvious use case is allowing insecure-registry access to self hosted registries.
        ## Can be ipaddress and domain_name.
        ## example define mirror.registry.io or 172.19.16.11:5000
        ## set "name": "url". insecure url must be started http://
        ## Port number is also needed if the default HTTPS port is not used.
        containerd_insecure_registries:
          "registry.ark.svc.ops.openark": "http://registry.ark.svc.{{ cluster_name }}" # DevSkim: ignore DS137138

        ## Settings for etcd deployment type
        # Set this to docker if you are using container_manager: docker
        etcd_deployment_type: "{{ 'docker' if container_manager == 'docker' else 'host' }}" # data is stored in /opt/etcd
        # Directory where etcd data stored
        etcd_data_dir: /opt/etcd
        etcd_config_dir: /etc/etcd
        etcd_events_data_dir: /var/lib/etcd-events

        # Choose network plugin (cilium, calico, kube-ovn, weave or flannel. Use cni for generic cni plugin)
        # Can also be set to 'cloud', which lets the cloud provider setup appropriate routing
        kube_network_plugin: cilium

        # Setting multi_networking to true will install Multus: https://github.com/intel/multus-cni
        kube_network_plugin_multus: false

        # internal network. When used, it will assign IP
        # addresses from this range to individual pods.
        # This network must be unused in your network infrastructure!
        kube_pods_subnet: 10.48.0.0/12
        kube_child_pods_subnet: 10.96.0.0/12

        # internal network node size allocation (optional). This is the size allocated
        # to each node for pod IP address allocation. Note that the number of pods per node is
        # also limited by the kubelet_max_pods variable which defaults to 110.
        #
        # Example:
        # Up to 64 nodes and up to 254 or kubelet_max_pods (the lowest of the two) pods per node:
        #  - kube_pods_subnet: 10.233.64.0/18
        #  - kube_network_node_prefix: 24
        #  - kubelet_max_pods: 110
        #
        # Example:
        # Up to 128 nodes and up to 126 or kubelet_max_pods (the lowest of the two) pods per node:
        #  - kube_pods_subnet: 10.233.64.0/18
        #  - kube_network_node_prefix: 25
        #  - kubelet_max_pods: 110
        kube_network_node_prefix: 24

        # Kubernetes internal network for services, unused block of space.
        kube_service_addresses: 10.64.0.0/12 # same as CNI CIDR
        kube_child_service_addresses: 10.112.0.0/12 # same as CNI CIDR

        ## Kube Proxy mode One of ['iptables','ipvs']
        kube_proxy_mode: ipvs

        # Configure Dual Stack networking (i.e. both IPv4 and IPv6)
        enable_dual_stack_networks: false # disable IPv6

        # configure arp_ignore and arp_announce to avoid answering ARP queries from kube-ipvs0 interface
        # must be set to true for MetalLB to work
        kube_proxy_strict_arp: true # set to true for MetalLB to work

        # DNS configuration.
        # Kubernetes cluster name, also will be used as DNS domain
        cluster_name: ops.openark # append `ops.` to prevent `openark.` from begin TLD

        ## Upstream dns servers
        upstream_dns_servers:
          - 1.1.1.1
          - 1.0.0.1

        # Enable nodelocal dns cache
        enable_nodelocaldns: false

        # Enable k8s_external plugin for CoreDNS
        enable_coredns_k8s_external: false
        # Enable endpoint_pod_names option for kubernetes plugin
        enable_coredns_k8s_endpoint_pod_names: false

        # nginx-proxy configure
        nginx_config_dir: /etc/nginx

        # krew root dir
        krew_root_dir: /usr/local/krew

        # sysctl_file_path to add sysctl conf to
        sysctl_file_path: /etc/sysctl.d/99-sysctl.conf

        #############################
        # CNI / Calico
        #############################

        # add default ippool blockSize (defaults kube_network_node_prefix)
        # calico_pool_blocksize: "{{ kube_network_node_prefix }}"
        # Global as_num (/calico/bgp/v1/global/as_num)
        global_as_num: "64512"
        # You can set MTU value here. If left undefined or empty, it will
        # not be specified in calico CNI config, so Calico will use built-in
        # defaults. The value should be a number, not a string.
        calico_mtu: 9000
        # Choose data store type for calico: "etcd" or "kdd" (kubernetes datastore)
        # The default value for calico_datastore is set in role kubespray-default
        calico_datastore: kdd
        # Calico container settings
        calico_allow_ip_forwarding: false
        # Should calico ignore kernel's RPF check setting,
        # see https://github.com/projectcalico/felix/blob/ab8799eaea66627e5db7717e62fca61fd9c08646/python/calico/felix/config.py#L198
        calico_node_ignorelooserpf: false
        # Advertise Cluster IPs
        calico_advertise_cluster_ips: true
        # Advertise Service LoadBalancer IPs
        calico_advertise_service_loadbalancer_ips:
          - 192.168.0.0/24 # Dev
        # Configure peering with router(s) at global scope
        peer_with_router: false
        # Set calico network backend: "bird", "vxlan" or "none"
        # bird enable BGP routing, required for ipip and no encapsulation modes
        calico_network_backend: bird # enable BGP routing
        # Enable BGP encapsulation mode
        calico_ipip_mode: CrossSubnet
        calico_vxlan_mode: Never
        # Enable eBPF mode
        calico_bpf_enabled: false

        #############################
        # CNI / Cilium
        #############################

        # Log-level
        cilium_debug: false

        cilium_enable_ipv4: true
        cilium_enable_ipv6: false

        # Enable l2 announcement from cilium to replace Metallb Ref: https://docs.cilium.io/en/v1.14/network/l2-announcements/
        cilium_l2announcements: false

        # Identity allocation mode selects how identities are shared between cilium
        # nodes by setting how they are stored. The options are "crd" or "kvstore".
        # - "crd" stores identities in kubernetes as CRDs (custom resource definition).
        #   These can be queried with:
        #     `kubectl get ciliumid`
        # - "kvstore" stores identities in an etcd kvstore.
        # - In order to support External Workloads, "crd" is required
        #   - Ref: https://docs.cilium.io/en/stable/gettingstarted/external-workloads/#setting-up-support-for-external-workloads-beta
        # - KVStore operations are only required when cilium-operator is running with any of the below options:
        #   - --synchronize-k8s-services
        #   - --synchronize-k8s-nodes
        #   - --identity-allocation-mode=kvstore
        #   - Ref: https://docs.cilium.io/en/stable/internals/cilium_operator/#kvstore-operations
        cilium_identity_allocation_mode: kvstore

        # Overlay Network Mode
        cilium_tunnel_mode: disabled  # native

        # LoadBalancer Mode (snat/dsr/hybrid) Ref: https://docs.cilium.io/en/stable/network/kubernetes/kubeproxy-free/#dsr-mode
        cilium_loadbalancer_mode: hybrid

        # Optional features
        cilium_enable_prometheus: true
        # Enable if you want to make use of hostPort mappings
        cilium_enable_portmap: true
        # Monitor aggregation level (none/low/medium/maximum)
        cilium_monitor_aggregation: medium
        # Kube Proxy Replacement mode (strict/partial)
        cilium_kube_proxy_replacement: strict

        # Auto direct nodes routes can be used to advertise pods routes in your cluster
        # without any tunelling (with `cilium_tunnel_mode` sets to `disabled`).
        # This works only if you have a L2 connectivity between all your nodes.
        # You wil also have to specify the variable `cilium_native_routing_cidr` to
        # make this work. Please refer to the cilium documentation for more
        # information about this kind of setups.
        cilium_auto_direct_node_routes: false

        # Allows to explicitly specify the IPv4 CIDR for native routing.
        # When specified, Cilium assumes networking for this CIDR is preconfigured and
        # hands traffic destined for that range to the Linux network stack without
        # applying any SNAT.
        # Generally speaking, specifying a native routing CIDR implies that Cilium can
        # depend on the underlying networking stack to route packets to their
        # destination. To offer a concrete example, if Cilium is configured to use
        # direct routing and the Kubernetes CIDR is included in the native routing CIDR,
        # the user must configure the routes to reach pods, either manually or by
        # setting the auto-direct-node-routes flag.
        cilium_native_routing_cidr: "10.0.0.0/8"

        # Allows to explicitly specify the IPv6 CIDR for native routing.
        cilium_native_routing_cidr_ipv6: "fd00::/100"

        # Enable transparent network encryption.
        cilium_encryption_enabled: false

        # Encryption method. Can be either ipsec or wireguard.
        # Only effective when `cilium_encryption_enabled` is set to true.
        cilium_encryption_type: wireguard

        # Enable encryption for pure node to node traffic.
        # This option is only effective when `cilium_encryption_type` is set to `ipsec`.
        cilium_ipsec_node_encryption: false

        # If your kernel or distribution does not support WireGuard, Cilium agent can be configured to fall back on the user-space implementation.
        # When this flag is enabled and Cilium detects that the kernel has no native support for WireGuard,
        # it will fallback on the wireguard-go user-space implementation of WireGuard.
        # This option is only effective when `cilium_encryption_type` is set to `wireguard`.
        cilium_wireguard_userspace_fallback: false

        # Enable Bandwidth Manager
        # Cilium’s bandwidth manager supports the kubernetes.io/egress-bandwidth Pod annotation.
        # Bandwidth enforcement currently does not work in combination with L7 Cilium Network Policies.
        # In case they select the Pod at egress, then the bandwidth enforcement will be disabled for those Pods.
        # Bandwidth Manager requires a v5.1.x or more recent Linux kernel.
        cilium_enable_bandwidth_manager: true

        # IP Masquerade Agent
        # https://docs.cilium.io/en/stable/concepts/networking/masquerading/
        # By default, all packets from a pod destined to an IP address outside of the cilium_native_routing_cidr range are masqueraded
        cilium_ip_masq_agent_enable: false

        # Hubble
        ### Enable Hubble without install
        cilium_enable_hubble: true
        ### Enable Hubble Metrics
        cilium_enable_hubble_metrics: true
        ### Enable Hubble install
        cilium_hubble_install: true
        ### Enable auto generate certs if cilium_hubble_install: true
        cilium_hubble_tls_generate: true

        # The default IP address management mode is "Cluster Scope".
        # https://docs.cilium.io/en/stable/concepts/networking/ipam/
        cilium_ipam_mode: cluster-pool

        ## A dictionary of extra config variables to add to cilium-config, formatted like:
        ##  cilium_config_extra_vars:
        ##    var1: "value1"
        ##    var2: "value2"
        cilium_config_extra_vars:
          # NOTE: See more performance tuning guide at: https://docs.cilium.io/en/stable/operations/performance/tuning/
          bpf-lb-acceleration: "best-effort"  # [disabled, native, best-effort]
          # Enable on cilium >= 1.16
          # datapath-mode: "{{ 'netkit' if (kiss_hpc_enable | default(False)) else 'veth' }}"
          enable-bbr: "true"
          enable-ipv4-big-tcp: "true"
          enable-ipv6-big-tcp: "true"

        # Name of the cluster. Only relevant when building a mesh of clusters.
        cilium_cluster_name: "{{ cluster_name }}"

        # -- Enables masquerading of IPv4 traffic leaving the node from endpoints.
        # Available for Cilium v1.10 and up
        cilium_enable_ipv4_masquerade: true
        # -- Enables masquerading of IPv6 traffic leaving the node from endpoints.
        # Available for Cilium v1.10 and up
        cilium_enable_ipv6_masquerade: true

        # -- Enable native IP masquerade support in eBPF
        cilium_enable_bpf_masquerade: "{{ kiss_hpc_enable | default(False) }}"

        # -- Configure whether direct routing mode should route traffic via
        # host stack (true) or directly and more efficiently out of BPF (false) if
        # the kernel supports it. The latter has the implication that it will also
        # bypass netfilter in the host namespace.
        cilium_enable_host_legacy_routing: false  # BPF, depends on BBR

        # A list of extra rules variables to add to clusterrole for cilium operator, formatted like:
        #   cilium_clusterrole_rules_operator_extra_vars:
        #     - apiGroups:
        #       - '""'
        #       resources:
        #       - pods
        #       verbs:
        #       - delete
        #     - apiGroups:
        #       - '""'
        #       resources:
        #       - nodes
        #       verbs:
        #       - list
        #       - watch
        #       resourceNames:
        #       - toto
        cilium_clusterrole_rules_operator_extra_vars:
          - apiGroups:
              - cilium.io
            resources:
              - ciliumpodippools
            verbs:
              - create
              - get
              - list
              - update
              - watch

        #############################
        # addons
        #############################

        # Kubernetes dashboard
        # RBAC required. see docs/getting-started.md for access details.
        dashboard_enabled: false

        # Helm deployment
        helm_enabled: true

        # Metrics Server deployment
        metrics_server_enabled: true

        # Local volume provisioner deployment
        local_volume_provisioner_enabled: false
        # local_volume_provisioner_storage_classes:
        #   local-storage:
        #     host_dir: /mnt/disks
        #     mount_dir: /mnt/disks
        #     volume_mode: Filesystem
        #     fs_type: ext4
        #   fast-disks:
        #     host_dir: /mnt/fast-disks
        #     mount_dir: /mnt/fast-disks
        #     block_cleaner_command:
        #       - "/scripts/shred.sh"
        #       - "2"
        #     volume_mode: Filesystem
        #     fs_type: ext4

        # Cert manager deployment
        cert_manager_enabled: true
        cert_manager_namespace: cert-manager

        # MetalLB deployment
        metallb_enabled: "{{ (kube_network_plugin | default('')) == 'calico' }}"
        metallb_speaker_enabled: false
        metallb_namespace: metallb-system
        metallb_config:
          address_pools:
            dev:
              ip_range:
                - 192.168.0.0/24 # for Development
              auto_assign: true
              avoid_buggy_ips: true
          layer3:
            defaults:
              peer_port: 179 # The TCP port to talk to. Defaults to 179, you shouldn't need to set this in production.
              hold_time: 120s # Requested BGP hold time, per RFC4271.
            # borrow calico BGP peers
            metallb_peers: {}

        #############################
        # Hardening
        #############################

        ## kube-apiserver
        authorization_modes:
          - Node
          - RBAC
        # AppArmor-based OS
        # kube_apiserver_feature_gates: ['AppArmor=true']
        kube_apiserver_request_timeout: 120s
        kube_apiserver_service_account_lookup: true

        # enable kubernetes audit
        kubernetes_audit: true
        audit_log_path: /var/log/kube-apiserver-log.json
        audit_log_maxage: 30
        audit_log_maxbackups: 10
        audit_log_maxsize: 100

        tls_min_version: VersionTLS12
        tls_cipher_suites:
          - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256
          - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
          - TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305

        # enable encryption at rest
        kube_encrypt_secret_data: true
        kube_encryption_resources:
          - secrets
        kube_encryption_algorithm: secretbox

        kube_apiserver_enable_admission_plugins:
          - EventRateLimit
          - AlwaysPullImages
          - ServiceAccount
          - NamespaceLifecycle
          - NodeRestriction
          - LimitRanger
          - ResourceQuota
          - MutatingAdmissionWebhook
          - ValidatingAdmissionWebhook
          - PodNodeSelector
          # - PodSecurity
        kube_apiserver_admission_control_config_file: true
        # EventRateLimit plugin configuration
        kube_apiserver_admission_event_rate_limits:
          limit_1:
            type: Namespace
            qps: 50
            burst: 100
            cache_size: 2000
          limit_2:
            type: User
            qps: 50
            burst: 100
        kube_profiling: false

        ## kube-controller-manager
        kube_controller_manager_bind_address: 127.0.0.1
        kube_controller_terminated_pod_gc_threshold: 50
        # AppArmor-based OS
        # kube_controller_feature_gates: ["RotateKubeletServerCertificate=true", "AppArmor=true"]
        kube_controller_feature_gates:
          - RotateKubeletServerCertificate=true

        ## kube-scheduler
        kube_scheduler_bind_address: 127.0.0.1
        # AppArmor-based OS
        # kube_scheduler_feature_gates: ["AppArmor=true"]

        ## kubelet
        kubelet_authorization_mode_webhook: true
        kubelet_authentication_token_webhook: true
        kube_read_only_port: 0
        kubelet_rotate_server_certificates: true
        kubelet_protect_kernel_defaults: true
        kubelet_event_record_qps: 1
        kubelet_rotate_certificates: true
        kubelet_streaming_connection_idle_timeout: 5m
        kubelet_make_iptables_util_chains: true
        kubelet_feature_gates:
          - RotateKubeletServerCertificate=true
        kubelet_seccomp_default: true
        kubelet_systemd_hardening: false
        # In case you have multiple interfaces in your
        # control plane nodes and you want to specify the right
        # IP addresses, kubelet_secure_addresses allows you
        # to specify the IP from which the kubelet
        # will receive the packets.
        # kubelet_secure_addresses: 192.168.10.110 192.168.10.111 192.168.10.112

        # Fill values override here
        # See upstream https://github.com/postfinance/kubelet-csr-approver
        kubelet_csr_approver_values:
          bypassDnsResolution: true

        # additional configurations
        kube_owner: root
        kube_cert_group: root

        # create a default Pod Security Configuration and deny running of insecure pods
        # kube_system namespace is exempted by default
        kube_pod_security_use_default: false
        kube_pod_security_default_enforce: restricted

        #############################
        # upgrade
        #############################

        drain_nodes: false
        drain_pod_selector: serviceType!=ansible-task
        upgrade_node_fail_if_drain_fails: false
