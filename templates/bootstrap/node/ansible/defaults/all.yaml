---
all:
  children:
    kiss_ephemeral_node:
      vars: {}
    kube_control_plane:
    kube_node:
    calico_rr:
      hosts: {}
    etcd:
    k8s_cluster_default:
      vars: {}
    k8s_cluster:
      children:
        kube_control_plane:
        kube_node:
      vars:
        #############################
        # k8s-cluster
        #############################

        # Kubernetes configuration dirs and system namespace.
        # Those are where all the additional config stuff goes
        # the kubernetes normally puts in /srv/kubernetes.
        # This puts them in a sane location and namespace.
        # Editing those values will almost surely break something.
        kube_config_dir: /etc/kubernetes

        ### fail with swap on (default true)
        kubelet_fail_swap_on: true

        # An alternative flexvolume plugin directory
        kubelet_flexvolumes_plugins_dir: /var/lib/kubelet/volumeplugins

        ## Container runtime
        ## docker for docker, crio for cri-o and containerd for containerd.
        ## Additionally you can set this to kubeadm if you want to install etcd using kubeadm
        ## Kubeadm etcd deployment is experimental and only available for new deployments
        ## If this is not set, container manager will be inherited from the Kubespray defaults
        ## and not from k8s_cluster/k8s-cluster.yml, which might not be what you want.
        ## Also this makes possible to use different container manager for etcd nodes.
        container_manager: containerd

        containerd_base_runtime_spec_rlimit_nofile: 1048576

        # Containerd conf default dir
        containerd_storage_dir: "/var/lib/containerd"

        ## An obvious use case is allowing insecure-registry access to self hosted registries.
        ## Can be ipaddress and domain_name.
        ## example define mirror.registry.io or 172.19.16.11:5000
        ## set "name": "url". insecure url must be started http://
        ## Port number is also needed if the default HTTPS port is not used.
        containerd_insecure_registries:
          "registry.ark.svc.ops.openark": "http://registry.ark.svc.{{ cluster_name }}" # DevSkim: ignore DS137138

        ## Settings for etcd deployment type
        # Set this to docker if you are using container_manager: docker
        etcd_deployment_type: "{{ 'docker' if container_manager == 'docker' else 'host' }}" # data is stored in /opt/etcd
        # Directory where etcd data stored
        etcd_data_dir: /opt/etcd
        etcd_config_dir: /etc/etcd
        etcd_events_data_dir: /var/lib/etcd-events

        # Choose network plugin (cilium, calico, kube-ovn, weave or flannel. Use cni for generic cni plugin)
        # Can also be set to 'cloud', which lets the cloud provider setup appropriate routing
        kube_network_plugin: custom_cni # Cilium

        # Setting multi_networking to true will install Multus: https://github.com/intel/multus-cni
        kube_network_plugin_multus: false

        # internal network. When used, it will assign IP
        # addresses from this range to individual pods.
        # This network must be unused in your network infrastructure!
        kube_pods_subnet: 10.48.0.0/12
        kube_child_pods_subnet: 10.96.0.0/12

        # internal network node size allocation (optional). This is the size allocated
        # to each node for pod IP address allocation. Note that the number of pods per node is
        # also limited by the kubelet_max_pods variable which defaults to 110.
        #
        # Example:
        # Up to 64 nodes and up to 254 or kubelet_max_pods (the lowest of the two) pods per node:
        #  - kube_pods_subnet: 10.233.64.0/18
        #  - kube_network_node_prefix: 24
        #  - kubelet_max_pods: 110
        #
        # Example:
        # Up to 128 nodes and up to 126 or kubelet_max_pods (the lowest of the two) pods per node:
        #  - kube_pods_subnet: 10.233.64.0/18
        #  - kube_network_node_prefix: 25
        #  - kubelet_max_pods: 110
        kube_network_node_prefix: 24

        # Kubernetes internal network for services, unused block of space.
        kube_service_addresses: 10.64.0.0/12 # same as CNI CIDR
        kube_child_service_addresses: 10.112.0.0/12 # same as CNI CIDR

        ## Kube Proxy mode One of ['iptables','ipvs']
        kube_proxy_mode: ipvs

        # Configure Dual Stack networking (i.e. both IPv4 and IPv6)
        enable_dual_stack_networks: false # disable IPv6

        # configure arp_ignore and arp_announce to avoid answering ARP queries from kube-ipvs0 interface
        # must be set to true for MetalLB to work
        kube_proxy_strict_arp: "{{ (kube_network_plugin | default('')) == 'calico' }}" # set to true for MetalLB to work

        # DNS configuration.
        # Kubernetes cluster name, also will be used as DNS domain
        cluster_name: ops.openark # append `ops.` to prevent `openark.` from begin TLD

        ## Upstream dns servers
        upstream_dns_servers:
          - 1.1.1.1
          - 1.0.0.1

        # Enable nodelocal dns cache
        enable_nodelocaldns: false

        # Enable k8s_external plugin for CoreDNS
        enable_coredns_k8s_external: false
        # Enable endpoint_pod_names option for kubernetes plugin
        enable_coredns_k8s_endpoint_pod_names: false

        # nginx-proxy configure
        nginx_config_dir: /etc/nginx

        # krew root dir
        krew_root_dir: /usr/local/krew

        # sysctl_file_path to add sysctl conf to
        sysctl_file_path: /etc/sysctl.d/99-sysctl.conf

        #############################
        # k8s-cluster / Extras
        #############################

        nameservers: "{{ upstream_dns_servers }}"
        systemd_resolved_disable_stub_listener: true

        #############################
        # CNI / Calico
        #############################

        # add default ippool blockSize (defaults kube_network_node_prefix)
        # calico_pool_blocksize: "{{ kube_network_node_prefix }}"
        # Global as_num (/calico/bgp/v1/global/as_num)
        global_as_num: "64512"
        # You can set MTU value here. If left undefined or empty, it will
        # not be specified in calico CNI config, so Calico will use built-in
        # defaults. The value should be a number, not a string.
        calico_mtu: 9000
        # Choose data store type for calico: "etcd" or "kdd" (kubernetes datastore)
        # The default value for calico_datastore is set in role kubespray-default
        calico_datastore: kdd
        # Calico container settings
        calico_allow_ip_forwarding: false
        # Should calico ignore kernel's RPF check setting,
        # see https://github.com/projectcalico/felix/blob/ab8799eaea66627e5db7717e62fca61fd9c08646/python/calico/felix/config.py#L198
        calico_node_ignorelooserpf: false
        # Advertise Cluster IPs
        calico_advertise_cluster_ips: true
        # Advertise Service LoadBalancer IPs
        calico_advertise_service_loadbalancer_ips:
          - 192.168.0.0/24 # Dev
        # Configure peering with router(s) at global scope
        peer_with_router: false
        # Set calico network backend: "bird", "vxlan" or "none"
        # bird enable BGP routing, required for ipip and no encapsulation modes
        calico_network_backend: bird # enable BGP routing
        # Enable BGP encapsulation mode
        calico_ipip_mode: CrossSubnet
        calico_vxlan_mode: Never
        # Enable eBPF mode
        calico_bpf_enabled: false

        #############################
        # CNI / Cilium
        #############################

        kube_proxy_remove: "{{ (kube_network_plugin | default('')) == 'custom_cni' }}" # set to true for Cilium to work

        custom_cni_chart_namespace: kube-system
        custom_cni_chart_release_name: cilium
        custom_cni_chart_repository_name: cilium
        custom_cni_chart_repository_url: https://helm.cilium.io/
        custom_cni_chart_ref: "{{ custom_cni_chart_repository_name }}/cilium"
        # custom_cni_chart_version: "{{ cilium_version[1:] }}"
        custom_cni_chart_version: "1.16.0"

        custom_cni_preflight_templates:
          - path: https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml
            state: present # options: [latest, present, absent]

        # Fill values override here
        custom_cni_chart_values:
          # -- Affinity for cilium-agent.
          # affinity:
          #   podAntiAffinity:
          #     requiredDuringSchedulingIgnoredDuringExecution:
          #       - topologyKey: kubernetes.io/hostname
          #         labelSelector:
          #           matchLabels:
          #             k8s-app: cilium

          # Configuration for types of authentication for Cilium (beta)
          authentication:
            # -- Enable authentication processing and garbage collection.
            # Note that if disabled, policy enforcement will still block requests that require authentication.
            # But the resulting authentication requests for these requests will not be processed, therefore the requests not be allowed.
            enabled: true

          # @schema
          # type: [boolean, string]
          # @schema
          # -- Enable installation of PodCIDR routes between worker
          # nodes if worker nodes share a common L2 network segment.
          autoDirectNodeRoutes: true

          # -- Enable bandwidth manager to optimize TCP and UDP workloads and allow
          # for rate-limiting traffic from individual Pods with EDT (Earliest Departure
          # Time) through the "kubernetes.io/egress-bandwidth" Pod annotation.
          bandwidthManager:
            # -- Enable bandwidth manager infrastructure (also prerequirement for BBR)
            enabled: true
            # -- Activate BBR TCP congestion control for Pods
            bbr: true

          bgp:
            # -- Enable BGP support inside Cilium; embeds a new ConfigMap for BGP inside
            # cilium-agent and cilium-operator
            enabled: false
            announce:
              # -- Enable allocation and announcement of service LoadBalancer IPs
              loadbalancerIP: true
              # -- Enable announcement of node pod CIDR
              podCIDR: false

          # CiliumBGPPeeringPolicy CRDs.
          bgpControlPlane:
            # -- Enables the BGP control plane.
            enabled: true

          bpf:
            # -- Enables pre-allocation of eBPF map values. This increases
            # memory usage but can reduce latency.
            # preallocateMaps: false
            # -- (string) Mode for Pod devices for the core datapath (veth, netkit, netkit-l2, lb-only)
            # @default -- `veth`
            datapathMode: netkit # FIXME: migrate OS from Rocky to Ubuntu that supports `CONFIG_NETKIT`
            # @schema
            # type: [null, boolean]
            # @schema
            # -- (bool) Enable native IP masquerade support in eBPF
            # @default -- `false`
            masquerade: true
            # @schema
            # type: [null, boolean]
            # @schema
            # -- (bool) Configure whether direct routing mode should route traffic via
            # host stack (true) or directly and more efficiently out of BPF (false) if
            # the kernel supports it. The latter has the implication that it will also
            # bypass netfilter in the host namespace.
            # @default -- `false`
            hostLegacyRouting: false
            # -- Allow cluster external access to ClusterIP services.
            lbExternalClusterIP: true

          clustermesh:
            # -- Deploy clustermesh-apiserver for clustermesh
            useAPIServer: false
            apiserver:
              tls:
                # -- Configure the clustermesh authentication mode.
                # Supported values:
                # - legacy:     All clusters access remote clustermesh instances with the same
                #               username (i.e., remote). The "remote" certificate must be
                #               generated with CN=remote if provided manually.
                # - migration:  Intermediate mode required to upgrade from legacy to cluster
                #               (and vice versa) with no disruption. Specifically, it enables
                #               the creation of the per-cluster usernames, while still using
                #               the common one for authentication. The "remote" certificate must
                #               be generated with CN=remote if provided manually (same as legacy).
                # - cluster:    Each cluster accesses remote etcd instances with a username
                #               depending on the local cluster name (i.e., remote-<cluster-name>).
                #               The "remote" certificate must be generated with CN=remote-<cluster-name>
                #               if provided manually. Cluster mode is meaningful only when the same
                #               CA is shared across all clusters part of the mesh.
                authMode: legacy
                # -- Configure automatic TLS certificates generation.
                # A Kubernetes CronJob is used the generate any
                # certificates not provided by the user at installation
                # time.
                auto:
                  # -- When set to true, automatically generate a CA and certificates to
                  # enable mTLS between clustermesh-apiserver and external workload instances.
                  # If set to false, the certs to be provided by setting appropriate values below.
                  enabled: true
                  # Sets the method to auto-generate certificates. Supported values:
                  # - helm:         This method uses Helm to generate all certificates.
                  # - cronJob:      This method uses a Kubernetes CronJob the generate any
                  #                 certificates not provided by the user at installation
                  #                 time.
                  # - certmanager:  This method use cert-manager to generate & rotate certificates.
                  method: helm
              # clustermesh-apiserver Prometheus metrics configuration
              metrics:
                # -- Enables exporting apiserver metrics in OpenMetrics format.
                enabled: true
                serviceMonitor:
                  # -- Enable service monitor.
                  # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
                  enabled: true

          # -- Grafana dashboards for cilium-agent
          # grafana can import dashboards based on the label and value
          # ref: https://github.com/grafana/helm-charts/tree/main/charts/grafana#sidecar-for-dashboards
          dashboards:
            enabled: true

          debug:
            # -- Enable debug logging
            enabled: false

          egressGateway:
            # -- Enables egress gateway to redirect and SNAT the traffic that leaves the
            # cluster.
            enabled: true

          # -- Enables IPv4 BIG TCP support which increases maximum IPv4 GSO/GRO limits for nodes and pods
          enableIPv4BIGTCP: true
          # -- Enables IPv6 BIG TCP support which increases maximum IPv6 GSO/GRO limits for nodes and pods
          enableIPv6BIGTCP: true

          encryption:
            # -- Enable transparent network encryption.
            enabled: false
            # -- Encryption method. Can be either ipsec or wireguard.
            type: wireguard
            # -- Enable encryption for pure node to node traffic.
            # This option is only effective when encryption.type is set to "wireguard".
            nodeEncryption: false
            # -- Configure the WireGuard Pod2Pod strict mode.
            strictMode:
              # -- Enable WireGuard Pod2Pod strict mode.
              enabled: false
              # -- Allow dynamic lookup of remote node identities.
              # This is required when tunneling is used or direct routing is used and the node CIDR and pod CIDR overlap.
              allowRemoteNodeIdentities: false

          # Configure Cilium Envoy options.
          envoy:
            # @schema
            # type: [null, boolean]
            # @schema
            # -- Enable Envoy Proxy in standalone DaemonSet.
            # This field is enabled by default for new installation.
            # @default -- `true` for new installation
            enabled: true
            # -- Roll out cilium envoy pods automatically when configmap is updated.
            rollOutPods: true
            # -- Configure Cilium Envoy Prometheus options.
            # Note that some of these apply to either cilium-agent or cilium-envoy.
            prometheus:
              # -- Enable prometheus metrics for cilium-envoy
              enabled: true
              serviceMonitor:
                # -- Enable service monitors.
                # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
                # Note that this setting applies to both cilium-envoy _and_ cilium-agent
                # with Envoy enabled.
                enabled: true

          externalIPs:
            # -- Enable ExternalIPs service support.
            enabled: true

          gatewayAPI:
            # -- Enable support for Gateway API in cilium
            # This will automatically set enable-envoy-config as well.
            enabled: true
            # -- Enable proxy protocol for all GatewayAPI listeners. Note that _only_ Proxy protocol traffic will be accepted once this is enabled.
            enableProxyProtocol: false
            # -- Enable Backend Protocol selection support (GEP-1911) for Gateway API via appProtocol.
            enableAppProtocol: true
            # -- Enable ALPN for all listeners configured with Gateway API. ALPN will attempt HTTP/2, then HTTP 1.1.
            # Note that this will also enable `appProtocol` support, and services that wish to use HTTP/2 will need to indicate that via their `appProtocol`.
            enableAlpn: true

          hostPort:
            # -- Enable hostPort service support.
            enabled: true

          hubble:
            # -- Enable Hubble (true by default).
            enabled: true

            # -- Hubble metrics configuration.
            # See https://docs.cilium.io/en/stable/observability/metrics/#hubble-metrics
            # for more comprehensive documentation about Hubble metrics.
            metrics:
              # -- Enables exporting hubble metrics in OpenMetrics format.
              enableOpenMetrics: false
              tls:
                # Enable hubble metrics server TLS.
                enabled: false
              serviceMonitor:
                # -- Create ServiceMonitor resources for Prometheus Operator.
                # This requires the prometheus CRDs to be available.
                # ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
                enabled: true
              # -- Grafana dashboards for hubble
              # grafana can import dashboards based on the label and value
              # ref: https://github.com/grafana/helm-charts/tree/main/charts/grafana#sidecar-for-dashboards
              dashboards:
                enabled: true

            # -- Enables redacting sensitive information present in Layer 7 flows.
            redact:
              enabled: false
            # -- Whether Hubble should prefer to announce IPv6 or IPv4 addresses if both are available.
            preferIpv6: false
            peerService:
              # -- Service Port for the Peer service.
              # If not set, it is dynamically assigned to port 443 if TLS is enabled and to
              # port 80 if not.
              # servicePort: 80
              # -- Target Port for the Peer service, must match the hubble.listenAddress'
              # port.
              targetPort: 4244
              # -- The cluster domain to use to query the Hubble Peer service. It should
              # be the local cluster.
              clusterDomain: "{{ cluster_name }}"
            # -- TLS configuration for Hubble
            tls:
              # -- Enable mutual TLS for listenAddress. Setting this value to false is
              # highly discouraged as the Hubble API provides access to potentially
              # sensitive network flow metadata and is exposed on the host network.
              enabled: true
              # -- Configure automatic TLS certificates generation.
              auto:
                # -- Auto-generate certificates.
                # When set to true, automatically generate a CA and certificates to
                # enable mTLS between Hubble server and Hubble Relay instances. If set to
                # false, the certs for Hubble server need to be provided by setting
                # appropriate values below.
                enabled: true
                # -- Set the method to auto-generate certificates. Supported values:
                # - helm:         This method uses Helm to generate all certificates.
                # - cronJob:      This method uses a Kubernetes CronJob the generate any
                #                 certificates not provided by the user at installation
                #                 time.
                # - certmanager:  This method use cert-manager to generate & rotate certificates.
                method: helm
            relay:
              # -- Enable Hubble Relay (requires hubble.enabled=true)
              enabled: true
              # -- Roll out Hubble Relay pods automatically when configmap is updated.
              rollOutPods: true
              # -- Enable prometheus metrics for hubble-relay on the configured port at
              # /metrics
              prometheus:
                enabled: true
                serviceMonitor:
                  # -- Enable service monitors.
                  # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
                  enabled: true

            ui:
              # -- Whether to enable the Hubble UI.
              enabled: true
              # -- Roll out Hubble-ui pods automatically when configmap is updated.
              rollOutPods: true
              standalone:
                # -- When true, it will allow installing the Hubble UI only, without checking dependencies.
                # It is useful if a cluster already has cilium and Hubble relay installed and you just
                # want Hubble UI to be deployed.
                # When installed via helm, installing UI should be done via `helm upgrade` and when installed via the cilium cli, then `cilium hubble enable --ui`
                enabled: false

          ingressController:
            # -- Enable cilium ingress controller
            # This will automatically set enable-envoy-config as well.
            enabled: false
            # -- Set cilium ingress controller to be the default ingress controller
            # This will let cilium ingress controller route entries without ingress class set
            default: true
            # -- Default ingress load balancer mode
            # Supported values: shared, dedicated
            # For granular control, use the following annotations on the ingress resource:
            # "ingress.cilium.io/loadbalancer-mode: dedicated" (or "shared").
            loadbalancerMode: dedicated
            # -- Enforce https for host having matching TLS host in Ingress.
            # Incoming traffic to http listener will return 308 http error code with respective location in header.
            enforceHttps: true
            # -- Enable proxy protocol for all Ingress listeners. Note that _only_ Proxy protocol traffic will be accepted once this is enabled.
            enableProxyProtocol: false

          ipam:
            # -- Configure IP Address Management mode.
            # ref: https://docs.cilium.io/en/stable/network/concepts/ipam/
            mode: "cluster-pool"

            operator:
              # @schema
              # type: [array, string]
              # @schema
              # -- IPv4 CIDR list range to delegate to individual nodes for IPAM.
              clusterPoolIPv4PodCIDRList:
                - "{{ kube_pods_subnet }}"

              # -- IPv4 CIDR mask size to delegate to individual nodes for IPAM.
              clusterPoolIPv4MaskSize: "{{ kube_network_node_prefix }}"

              # @schema
              # type: [array, string]
              # @schema
              # -- IPv6 CIDR list range to delegate to individual nodes for IPAM.
              clusterPoolIPv6PodCIDRList:
                - "{{ kube_pods_subnet_ipv6 }}"

              # -- IPv6 CIDR mask size to delegate to individual nodes for IPAM.
              clusterPoolIPv6MaskSize: "{{ kube_network_node_prefix_ipv6 }}"

          # -- Configure the eBPF-based ip-masq-agent
          ipMasqAgent:
            enabled: true

          ipv4:
            # -- Enable IPv4 support.
            enabled: true

          # -- (string) Allows to explicitly specify the IPv4 CIDR for native routing.
          # When specified, Cilium assumes networking for this CIDR is preconfigured and
          # hands traffic destined for that range to the Linux network stack without
          # applying any SNAT.
          # Generally speaking, specifying a native routing CIDR implies that Cilium can
          # depend on the underlying networking stack to route packets to their
          # destination. To offer a concrete example, if Cilium is configured to use
          # direct routing and the Kubernetes CIDR is included in the native routing CIDR,
          # the user must configure the routes to reach pods, either manually or by
          # setting the auto-direct-node-routes flag.
          ipv4NativeRoutingCIDR: "10.32.0.0/12"

          ipv6:
            # -- Enable IPv6 support.
            enabled: false

          # -- (string) Allows to explicitly specify the IPv6 CIDR for native routing.
          # When specified, Cilium assumes networking for this CIDR is preconfigured and
          # hands traffic destined for that range to the Linux network stack without
          # applying any SNAT.
          # Generally speaking, specifying a native routing CIDR implies that Cilium can
          # depend on the underlying networking stack to route packets to their
          # destination. To offer a concrete example, if Cilium is configured to use
          # direct routing and the Kubernetes CIDR is included in the native routing CIDR,
          # the user must configure the routes to reach pods, either manually or by
          # setting the auto-direct-node-routes flag.
          ipv6NativeRoutingCIDR: "fd00::/100"

          # -- Configure Kubernetes specific configuration
          k8s:
            # -- requireIPv4PodCIDR enables waiting for Kubernetes to provide the PodCIDR
            # range via the Kubernetes node resource
            requireIPv4PodCIDR: false
            # -- requireIPv6PodCIDR enables waiting for Kubernetes to provide the PodCIDR
            # range via the Kubernetes node resource
            requireIPv6PodCIDR: false

          # -- (string) Kubernetes service host - use "auto" for automatic lookup from the cluster-info ConfigMap (kubeadm-based clusters only)
          k8sServiceHost: "{{ kube_apiserver_global_endpoint | urlsplit('hostname') }}"
          # @schema
          # type: [string, integer]
          # @schema
          # -- (string) Kubernetes service port
          k8sServicePort: "{{ kube_apiserver_global_endpoint | urlsplit('port') }}"

          # -- Configure the kube-proxy replacement in Cilium BPF datapath
          # Valid options are "true" or "false".
          # ref: https://docs.cilium.io/en/stable/network/kubernetes/kubeproxy-free/
          kubeProxyReplacement: "{{ kube_proxy_remove }}"

          # -- Configure L2 announcements
          l2announcements:
            # -- Enable L2 announcements
            enabled: false

          l2NeighDiscovery:
            # -- Enable L2 neighbor discovery in the agent
            enabled: true

          # -- Configure L2 pod announcements
          l2podAnnouncements:
            # -- Enable L2 pod announcements
            enabled: false

          # -- Enable Layer 7 network policy.
          l7Proxy: true

          # -- Configure service load balancing
          loadBalancer:
            # -- mode is the operation mode of load balancing for remote backends
            # e.g. snat, dsr, hybrid
            mode: dsr

            # -- acceleration is the option to accelerate service handling via XDP
            # Applicable values can be: disabled (do not use XDP), native (XDP BPF
            # program is run directly out of the networking driver's early receive
            # path), or best-effort (use native mode XDP acceleration on devices
            # that support it).
            acceleration: best-effort

            # -- L7 LoadBalancer
            l7:
              # -- Enable L7 service load balancing via envoy proxy.
              # The request to a k8s service, which has specific annotation e.g. service.cilium.io/lb-l7,
              # will be forwarded to the local backend proxy to be load balanced to the service endpoints.
              # Please refer to docs for supported annotations for more configuration.
              #
              # Applicable values:
              #   - envoy: Enable L7 load balancing via envoy proxy. This will automatically set enable-envoy-config as well.
              #   - disabled: Disable L7 load balancing by way of service annotation.
              backend: envoy

          # -- Enable Local Redirect Policy.
          localRedirectPolicy: true # required for node-local-dns

          # -- Enables periodic logging of system load
          logSystemLoad: false

          monitor:
            # -- Enable the cilium-monitor sidecar.
            enabled: false

          nodeinit:
            # -- Enable the node initialization DaemonSet
            enabled: false

          nodeIPAM:
            # -- Configure Node IPAM
            # ref: https://docs.cilium.io/en/stable/network/node-ipam/
            enabled: false

          # -- Configure N-S k8s service loadbalancing
          nodePort:
            # -- Enable the Cilium NodePort service implementation.
            enabled: true

          operator:
            # -- Enable the cilium-operator component (required).
            enabled: true
            # -- Roll out cilium-operator pods automatically when configmap is updated.
            rollOutPods: true
            # -- Affinity for cilium-operator
            # affinity:
            #   podAntiAffinity:
            #     requiredDuringSchedulingIgnoredDuringExecution:
            #       - topologyKey: kubernetes.io/hostname
            #         labelSelector:
            #           matchLabels:
            #             io.cilium/app: operator
            # -- Enable prometheus metrics for cilium-operator on the configured port at
            # /metrics
            prometheus:
              enabled: true
              serviceMonitor:
                # -- Enable service monitors.
                # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
                enabled: true
            # -- Grafana dashboards for cilium-operator
            # grafana can import dashboards based on the label and value
            # ref: https://github.com/grafana/helm-charts/tree/main/charts/grafana#sidecar-for-dashboards
            dashboards:
              enabled: true

          # @schema
          # type: [null, string, array]
          # @schema
          # -- policyCIDRMatchMode is a list of entities that may be selected by CIDR selector.
          # The possible value is "nodes".
          policyCIDRMatchMode:
            - nodes

          preflight:
            # -- Enable Cilium pre-flight resources (required for upgrade)
            enabled: false

          # -- Configure prometheus metrics on the configured port at /metrics
          prometheus:
            enabled: true
            serviceMonitor:
              # -- Enable service monitors.
              # This requires the prometheus CRDs to be available (see https://github.com/prometheus-operator/prometheus-operator/blob/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml)
              enabled: true
              # -- Set to `true` and helm will not check for monitoring.coreos.com/v1 CRDs before deploying
              trustCRDsExist: true

          # -- Enable native-routing mode or tunneling mode.
          # Possible values:
          #   - ""
          #   - native
          #   - tunnel
          # @default -- `"tunnel"`
          routingMode: native

          # -- SCTP Configuration Values
          sctp:
            # -- Enable SCTP support. NOTE: Currently, SCTP support does not support rewriting ports or multihoming.
            enabled: false

          # -- Configure sysctl override described in #20072.
          sysctlfix:
            # -- Enable the sysctl override. When enabled, the init container will mount the /proc of the host so that the `sysctlfix` utility can execute.
            enabled: true

        #############################
        # addons
        #############################

        # Kubernetes dashboard
        # RBAC required. see docs/getting-started.md for access details.
        dashboard_enabled: false

        # Helm deployment
        helm_enabled: true

        # Metrics Server deployment
        metrics_server_enabled: true

        # Local volume provisioner deployment
        local_volume_provisioner_enabled: false
        # local_volume_provisioner_storage_classes:
        #   local-storage:
        #     host_dir: /mnt/disks
        #     mount_dir: /mnt/disks
        #     volume_mode: Filesystem
        #     fs_type: ext4
        #   fast-disks:
        #     host_dir: /mnt/fast-disks
        #     mount_dir: /mnt/fast-disks
        #     block_cleaner_command:
        #       - "/scripts/shred.sh"
        #       - "2"
        #     volume_mode: Filesystem
        #     fs_type: ext4

        # Cert manager deployment
        cert_manager_enabled: true
        cert_manager_namespace: cert-manager

        # MetalLB deployment
        metallb_enabled: "{{ (kube_network_plugin | default('')) == 'calico' }}"
        metallb_speaker_enabled: false
        metallb_namespace: metallb-system
        metallb_config:
          address_pools:
            dev:
              ip_range:
                - 192.168.0.0/24 # for Development
              auto_assign: true
              avoid_buggy_ips: true
          layer3:
            defaults:
              peer_port: 179 # The TCP port to talk to. Defaults to 179, you shouldn't need to set this in production.
              hold_time: 120s # Requested BGP hold time, per RFC4271.
            # borrow calico BGP peers
            metallb_peers: {}

        #############################
        # Hardening
        #############################

        ## kube-apiserver
        authorization_modes:
          - Node
          - RBAC
        # AppArmor-based OS
        kube_apiserver_feature_gates:
          - AppArmor=true
        kube_apiserver_request_timeout: 120s
        kube_apiserver_service_account_lookup: true

        # enable kubernetes audit
        kubernetes_audit: true
        audit_log_path: /var/log/kube-apiserver-log.json
        audit_log_maxage: 30
        audit_log_maxbackups: 10
        audit_log_maxsize: 100

        tls_min_version: VersionTLS12
        tls_cipher_suites:
          - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256
          - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
          - TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305

        # enable encryption at rest
        kube_encrypt_secret_data: true
        kube_encryption_resources:
          - secrets
        kube_encryption_algorithm: secretbox

        kube_apiserver_enable_admission_plugins:
          - EventRateLimit
          - AlwaysPullImages
          - ServiceAccount
          - NamespaceLifecycle
          - NodeRestriction
          - LimitRanger
          - ResourceQuota
          - MutatingAdmissionWebhook
          - ValidatingAdmissionWebhook
          - PodNodeSelector
          # - PodSecurity
        kube_apiserver_admission_control_config_file: true
        # EventRateLimit plugin configuration
        kube_apiserver_admission_event_rate_limits:
          limit_1:
            type: Namespace
            qps: 50
            burst: 100
            cache_size: 2000
          limit_2:
            type: User
            qps: 50
            burst: 100
        kube_profiling: false

        ## kube-controller-manager
        kube_controller_manager_bind_address: 127.0.0.1
        kube_controller_terminated_pod_gc_threshold: 50
        # AppArmor-based OS
        # kube_controller_feature_gates: ["RotateKubeletServerCertificate=true", "AppArmor=true"]
        kube_controller_feature_gates:
          - AppArmor=true
          - RotateKubeletServerCertificate=true

        ## kube-scheduler
        kube_scheduler_bind_address: 127.0.0.1
        # AppArmor-based OS
        kube_scheduler_feature_gates:
          - AppArmor=true

        ## kubelet
        kubelet_authorization_mode_webhook: true
        kubelet_authentication_token_webhook: true
        kube_read_only_port: 0
        kubelet_rotate_server_certificates: true
        kubelet_protect_kernel_defaults: true
        kubelet_event_record_qps: 1
        kubelet_rotate_certificates: true
        kubelet_streaming_connection_idle_timeout: 5m
        kubelet_make_iptables_util_chains: true
        kubelet_feature_gates:
          - RotateKubeletServerCertificate=true
        kubelet_seccomp_default: true
        kubelet_systemd_hardening: false
        # In case you have multiple interfaces in your
        # control plane nodes and you want to specify the right
        # IP addresses, kubelet_secure_addresses allows you
        # to specify the IP from which the kubelet
        # will receive the packets.
        # kubelet_secure_addresses: 192.168.10.110 192.168.10.111 192.168.10.112

        # Fill values override here
        # See upstream https://github.com/postfinance/kubelet-csr-approver
        kubelet_csr_approver_values:
          bypassDnsResolution: true

        # additional configurations
        kube_owner: root
        kube_cert_group: root

        # create a default Pod Security Configuration and deny running of insecure pods
        # kube_system namespace is exempted by default
        kube_pod_security_use_default: false
        kube_pod_security_default_enforce: restricted

        #############################
        # upgrade
        #############################

        drain_nodes: false
        drain_pod_selector: serviceType!=ansible-task
        upgrade_node_fail_if_drain_fails: false
