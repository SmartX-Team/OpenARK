---
# Configuration for the ingester
ingester:
  # -- Number of replicas for the ingester
  replicas: 2

  autoscaling:
    # -- Enable autoscaling for the ingester. WARNING: Autoscaling ingesters can result in lost data. Only do this if you know what you're doing.
    enabled: false
    # -- Minimum autoscaling replicas for the ingester
    minReplicas: 2 # minimum: 2
    # -- Maximum autoscaling replicas for the ingester
    maxReplicas: 3
    # -- Autoscaling behavior configuration for the ingester
    behavior: {}
    # -- Target CPU utilisation percentage for the ingester
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilisation percentage for the ingester
    targetMemoryUtilizationPercentage:

  # -- Affinity for ingester pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Soft node and soft zone anti-affinity
  affinity: |
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        # KISS normal control plane nodes should be preferred
        - weight: 1
          preference:
            matchExpressions:
              - key: node-role.kubernetes.io/kiss-ephemeral-control-plane
                operator: DoesNotExist
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: node-role.kubernetes.io/kiss
                operator: In
                values:
                  - ControlPlane
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                {{- include "tempo.selectorLabels" (dict "ctx" . "component" "ingester") | nindent 12 }}
            topologyKey: kubernetes.io/hostname
        - weight: 75
          podAffinityTerm:
            labelSelector:
              matchLabels:
                {{- include "tempo.selectorLabels" (dict "ctx" . "component" "ingester") | nindent 12 }}
            topologyKey: topology.kubernetes.io/zone

  persistence:
    # -- Enable creating PVCs which is required when using boltdb-shipper
    enabled: false
    # -- Size of persistent or memory disk
    size: 10Gi
    # -- Storage class to be used.
    # If defined, storageClassName: <storageClass>.
    # If set to "-", storageClassName: "", which disables dynamic provisioning.
    # If empty or set to null, no storageClassName spec is
    # set, choosing the default provisioner (gp2 on AWS, standard on GKE, AWS, and OpenStack).
    storageClassName: ceph-block

# Configuration for the metrics-generator
metricsGenerator:
  # -- Specifies whether a metrics-generator should be deployed
  enabled: true
  # -- Number of replicas for the metrics-generator
  replicas: 1
  # -- Affinity for metrics-generator pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Hard node and soft zone anti-affinity
  affinity: |
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        # KISS normal control plane nodes should be preferred
        - weight: 1
          preference:
            matchExpressions:
              - key: node-role.kubernetes.io/kiss-ephemeral-control-plane
                operator: DoesNotExist
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: node-role.kubernetes.io/kiss
                operator: In
                values:
                  - ControlPlane
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              {{- include "tempo.selectorLabels" (dict "ctx" . "component" "metrics-generator") | nindent 10 }}
          topologyKey: kubernetes.io/hostname
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                {{- include "tempo.selectorLabels" (dict "ctx" . "component" "metrics-generator") | nindent 12 }}
            topologyKey: topology.kubernetes.io/zone
  # -- More information on configuration: https://grafana.com/docs/tempo/latest/configuration/#metrics-generator
  config:
    storage:
      remote_write:
        - url: http://kube-prometheus-stack-prometheus:9090/api/v1/write
          send_exemplars: false

# Configuration for the distributor
distributor:
  # -- Number of replicas for the distributor
  replicas: 1

  autoscaling:
    # -- Enable autoscaling for the distributor. WARNING: Autoscaling distributors can result in lost data. Only do this if you know what you're doing.
    enabled: false
    # -- Minimum autoscaling replicas for the distributor
    minReplicas: 1
    # -- Maximum autoscaling replicas for the distributor
    maxReplicas: 3
    # -- Autoscaling behavior configuration for the distributor
    behavior: {}
    # -- Target CPU utilisation percentage for the distributor
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilisation percentage for the distributor
    targetMemoryUtilizationPercentage:

  # -- Affinity for distributor pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Soft node and soft zone anti-affinity
  affinity: |
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        # KISS normal control plane nodes should be preferred
        - weight: 1
          preference:
            matchExpressions:
              - key: node-role.kubernetes.io/kiss-ephemeral-control-plane
                operator: DoesNotExist
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: node-role.kubernetes.io/kiss
                operator: In
                values:
                  - ControlPlane
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              {{- include "tempo.selectorLabels" (dict "ctx" . "component" "distributor") | nindent 10 }}
          topologyKey: kubernetes.io/hostname
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                {{- include "tempo.selectorLabels" (dict "ctx" . "component" "distributor") | nindent 12 }}
            topologyKey: topology.kubernetes.io/zone

# Configuration for the compactor
compactor:
  # -- Number of replicas for the compactor
  replicas: 1

  # -- Affinity for compactor pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Soft node and soft zone anti-affinity
  affinity: |
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        # KISS normal control plane nodes should be preferred
        - weight: 1
          preference:
            matchExpressions:
              - key: node-role.kubernetes.io/kiss-ephemeral-control-plane
                operator: DoesNotExist
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: node-role.kubernetes.io/kiss
                operator: In
                values:
                  - ControlPlane

# Configuration for the querier
querier:
  # -- Number of replicas for the querier
  replicas: 1

  autoscaling:
    # -- Enable autoscaling for the querier. WARNING: Autoscaling queriers can result in lost data. Only do this if you know what you're doing.
    enabled: false
    # -- Minimum autoscaling replicas for the querier
    minReplicas: 1
    # -- Maximum autoscaling replicas for the querier
    maxReplicas: 3
    # -- Autoscaling behavior configuration for the querier
    behavior: {}
    # -- Target CPU utilisation percentage for the querier
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilisation percentage for the querier
    targetMemoryUtilizationPercentage:

  # -- Affinity for querier pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Soft node and soft zone anti-affinity
  affinity: |
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        # KISS normal control plane nodes should be preferred
        - weight: 1
          preference:
            matchExpressions:
              - key: node-role.kubernetes.io/kiss-ephemeral-control-plane
                operator: DoesNotExist
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: node-role.kubernetes.io/kiss
                operator: In
                values:
                  - ControlPlane
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              {{- include "tempo.selectorLabels" (dict "ctx" . "component" "querier" "memberlist" true) | nindent 10 }}
          topologyKey: kubernetes.io/hostname
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                {{- include "tempo.selectorLabels" (dict "ctx" . "component" "querier" "memberlist" true) | nindent 12 }}
            topologyKey: topology.kubernetes.io/zone

# Configuration for the query-frontend
queryFrontend:
  query:
    # -- Required for grafana version <7.5 for compatibility with jaeger-ui. Doesn't work on ARM arch
    enabled: false

  # -- Number of replicas for the query-frontend
  replicas: 1

  autoscaling:
    # -- Enable autoscaling for the query-frontend
    enabled: false
    # -- Minimum autoscaling replicas for the query-frontend
    minReplicas: 1
    # -- Maximum autoscaling replicas for the query-frontend
    maxReplicas: 3
    # -- Autoscaling behavior configuration for the query-frontend
    behavior: {}
    # -- Target CPU utilisation percentage for the query-frontend
    targetCPUUtilizationPercentage: 60
    # -- Target memory utilisation percentage for the query-frontend
    targetMemoryUtilizationPercentage:

  ingress:
    # -- Specifies whether an ingress for the Jaeger should be created
    enabled: true
    # -- Ingress Class Name. MAY be required for Kubernetes versions >= 1.18
    # ingressClassName: nginx
    # -- Annotations for the Jaeger ingress
    annotations:
      cert-manager.io/cluster-issuer: example.com
      nginx.ingress.kubernetes.io/auth-url: https://$http_x_forwarded_auth/oauth2/auth
      nginx.ingress.kubernetes.io/auth-signin: https://$http_x_forwarded_auth/oauth2/start?rd=$http_x_forwarded_proto://$http_x_forwarded_host$escaped_request_uri
      nginx.ingress.kubernetes.io/cors-allow-origin: "*"
      nginx.ingress.kubernetes.io/enable-cors: "true"
      nginx.ingress.kubernetes.io/proxy-read-timeout: "3600"
      nginx.ingress.kubernetes.io/proxy-send-timeout: "3600"
      nginx.ingress.kubernetes.io/rewrite-target: /$1
      vine.ulagbulag.io/is-service: "true"
      vine.ulagbulag.io/is-service-public: "true"
      vine.ulagbulag.io/is-service-system: "false"
      vine.ulagbulag.io/service-kind: Grafana Tempo
    # -- Hosts configuration for the Jaeger ingress
    hosts:
      - host: query.tempo.example.com
        paths:
          - path: /dashboard/tempo/?(.*)
            # -- pathType (e.g. ImplementationSpecific, Prefix, .. etc.) might also be required by some Ingress Controllers
            pathType: Prefix
    # -- TLS configuration for the Jaeger ingress
    # tls:
    #   - secretName: tempo-query-tls
    #     hosts:
    #       - query.tempo.example.com

  # -- Affinity for query-frontend pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Hard node and soft zone anti-affinity
  affinity: |
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        # KISS normal control plane nodes should be preferred
        - weight: 1
          preference:
            matchExpressions:
              - key: node-role.kubernetes.io/kiss-ephemeral-control-plane
                operator: DoesNotExist
        # KISS gateway nodes should be more preferred
        - weight: 2
          preference:
            matchExpressions:
              - key: node-role.kubernetes.io/kiss
                operator: In
                values:
                  - Gateway
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: node-role.kubernetes.io/kiss
                operator: In
                values:
                  - ControlPlane
                  - Gateway
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              {{- include "tempo.selectorLabels" (dict "ctx" . "component" "query-frontend") | nindent 10 }}
          topologyKey: kubernetes.io/hostname
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                {{- include "tempo.selectorLabels" (dict "ctx" . "component" "query-frontend") | nindent 12 }}
            topologyKey: topology.kubernetes.io/zone

traces:
  jaeger:
    grpc:
      # -- Enable Tempo to ingest Jaeger GRPC traces
      enabled: false
      # -- Jaeger GRPC receiver config
      receiverConfig: {}
    thriftBinary:
      # -- Enable Tempo to ingest Jaeger Thrift Binary traces
      enabled: false
      # -- Jaeger Thrift Binary receiver config
      receiverConfig: {}
    thriftCompact:
      # -- Enable Tempo to ingest Jaeger Thrift Compact traces
      enabled: false
      # -- Jaeger Thrift Compact receiver config
      receiverConfig: {}
    thriftHttp:
      # -- Enable Tempo to ingest Jaeger Thrift HTTP traces
      enabled: false
      # -- Jaeger Thrift HTTP receiver config
      receiverConfig: {}
  zipkin:
    # -- Enable Tempo to ingest Zipkin traces
    enabled: false
    # -- Zipkin receiver config
    receiverConfig: {}
  otlp:
    http:
      # -- Enable Tempo to ingest Open Telemetry HTTP traces
      enabled: false
      # -- HTTP receiver advanced config
      receiverConfig: {}
    grpc:
      # -- Enable Tempo to ingest Open Telemetry GRPC traces
      enabled: true
      # -- GRPC receiver advanced config
      receiverConfig: {}
  opencensus:
    # -- Enable Tempo to ingest Open Census traces
    enabled: false
    # -- Open Census receiver config
    receiverConfig: {}
  # -- Enable Tempo to ingest traces from Kafka. Reference: https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/receiver/kafkareceiver
  kafka: {}

# To configure a different storage backend instead of local storage:
# storage:
#   trace:
#     backend: azure
#     azure:
#       container_name:
#       storage_account_name:
#       storage_account_key:
storage:
  trace:
    # Settings for the block storage backend and buckets.
    block:
      # -- The supported block versions are specified here https://grafana.com/docs/tempo/latest/configuration/parquet/
      version: null
      # -- Lis with dedicated attribute columns (only for vParquet3 or later)
      dedicated_columns: []
    # -- The supported storage backends are gcs, s3 and azure, as specified in https://grafana.com/docs/tempo/latest/configuration/#storage
    backend: local
    # The worker pool is used primarily when finding traces by id, but is also used by other.
    pool:
      # -- Total number of workers pulling jobs from the queue
      max_workers: 400
      # -- Length of job queue. imporatant for querier as it queues a job for every block it has to search
      queue_depth: 20000
  # Settings for the Admin client storage backend and buckets. Only valid is enterprise.enabled is true.
  admin:
    # -- The supported storage backends are gcs, s3 and azure, as specified in https://grafana.com/docs/enterprise-traces/latest/config/reference/#admin_client_config
    backend: filesystem

# Global overrides
global_overrides:
  metrics_generator_processors:
    - service-graphs

# memcached is for all of the Tempo pieces to coordinate with each other.
# you can use your self memcacherd by set enable: false and host + service
memcached:
  # -- Specified whether the memcached cachce should be enabled
  enabled: true
  # Number of replicas for memchached
  replicas: 1
  # -- Affinity for memcached pods. Passed through `tpl` and, thus, to be configured as string
  # @default -- Hard node and soft zone anti-affinity
  affinity: |
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        # KISS normal control plane nodes should be preferred
        - weight: 1
          preference:
            matchExpressions:
              - key: node-role.kubernetes.io/kiss-ephemeral-control-plane
                operator: DoesNotExist
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: node-role.kubernetes.io/kiss
                operator: In
                values:
                  - ControlPlane
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              {{- include "tempo.selectorLabels" (dict "ctx" . "component" "memcached") | nindent 10 }}
          topologyKey: kubernetes.io/hostname
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchLabels:
                {{- include "tempo.selectorLabels" (dict "ctx" . "component" "memcached") | nindent 12 }}
            topologyKey: topology.kubernetes.io/zone

# Configuration for the gateway
gateway:
  # -- Specifies whether the gateway should be enabled
  enabled: false
